{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from itertools import repeat\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utils import custom_data_loader, preprocess_data\n",
    "from Utils.SummaryWriter import LogSummary\n",
    "from Models.simpleFFBNN import SimpleFFBNN\n",
    "from Models.denseRegression import DenseRegressor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n",
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Function to get the device to be used for training the model\n",
    "    \"\"\"\n",
    "    cuda = torch.cuda.is_available()\n",
    "    print(\"CUDA Available: \", cuda)\n",
    "\n",
    "    if cuda:\n",
    "        gpu = GPUtil.getFirstAvailable()\n",
    "        print(\"GPU Available: \", gpu)\n",
    "        torch.cuda.set_device(gpu)\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(\"Device: \", device)\n",
    "    return device\n",
    "\n",
    "device = get_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Utils/config.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.X['savings'] = np.where(self.X['savings'] == 'low', 0, np.where(self.X['savings'] == 'medium', 1, 2))\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataloader_train, dataloader_test, dataloader_val = preprocess_data(pd.read_csv('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Data/quality_of_food.csv'), batch_size = 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFFBNN(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/trainedModels/simple_model.pth', map_location=torch.device('cpu'))\n",
    "#print(checkpoint)\n",
    "#model.load_state_dict(checkpoint['model'])\n",
    "model.load_state_dict(checkpoint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput():\n",
    "    def __init__(self, instances, batch_size, rounds):\n",
    "        self.T = instances\n",
    "        self.batch_size = batch_size\n",
    "        self.outputs = []\n",
    "        self.rounds = rounds\n",
    "        self.counter = 0\n",
    "\n",
    "\n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        if self.counter < 3:\n",
    "            sample_data = np.random.randint(self.batch_size)\n",
    "            #outs = module_out.view(self.batch_size, -1)\n",
    "            print(module_out.view(self.T, self.batch_size, -1).shape)\n",
    "            outs = module_out.view(self.T, self.batch_size, -1)[:, 0, :]\n",
    "            layer_size = outs.shape[1]\n",
    "\n",
    "            \n",
    "            write_summary.per_round_layer_output(layer_size, outs, self.rounds)\n",
    "\n",
    "            self.counter += 1\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_summary = LogSummary(name = 'Simple Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Utils/config.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.X['savings'] = np.where(self.X['savings'] == 'low', 0, np.where(self.X['savings'] == 'medium', 1, 2))\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataloader_train, dataloader_test, dataloader_val, dataset_train, dataset_test, dataset_val = preprocess_data(pd.read_csv('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Data/quality_of_food.csv'), batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class runActiveLearning():\n",
    "    def __init__(self, model_name, model, top_unc, dataloader_train, dataloader_test, dataloader_val, epochs, rounds, learning_rate, \n",
    "    batch_size, instances, seed_sample, retrain, resume_round, optimizer):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.top_unc = top_unc\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_test = dataloader_test\n",
    "        self.dataloader_val = dataloader_val\n",
    "        self.epochs = epochs\n",
    "        self.rounds = rounds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.instances = instances\n",
    "        self.seed_sample = seed_sample\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # a set of lists to store the selected indices with highest uncertainty\n",
    "        self.selected_data = set([])\n",
    "        # unexplored data\n",
    "        self.unexplored_data = set(range(len(dataloader_train)))\n",
    "\n",
    "        # make sure sklearn.metrics.r2_score is imported\n",
    "        #self.r2_score = r2_score\n",
    "\n",
    "\n",
    "    \n",
    "    def objective(self, output, target, kl, beta):\n",
    "        '''Objective function to calculate the loss function / KL divergence'''\n",
    "        loss_fun = nn.MSELoss()\n",
    "        discrimination_error = loss_fun(output.view(-1), target)\n",
    "        variational_bound = discrimination_error + beta * kl\n",
    "        return variational_bound, discrimination_error, kl\n",
    "\n",
    "    def get_entropy(self, y):\n",
    "        '''Function to calculate the entropy of the ensemble outputs'''\n",
    "        ensemble_probabilities = y.mean(0)\n",
    "        entropy = Categorical(probs = ensemble_probabilities).entropy()\n",
    "        return entropy\n",
    "\n",
    "\n",
    "\n",
    "    def get_validation_data(self, is_validation):\n",
    "        if not is_validation:\n",
    "            # train sampler randomly samples data from the selected data set\n",
    "            train_sampler = SubsetRandomSampler(list(self.selected_data))\n",
    "            # train loader will load the data from the train sampler\n",
    "            self.train_loader = DataLoader(self.dataloader_train, batch_size=self.batch_size, sampler=train_sampler, num_workers=1)\n",
    "\n",
    "        indices = list(self.unexplored_data)\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(0.1 * len(indices)))  # this line is to split the data into 90% training and 10% validation\n",
    "        validation_idx = np.random.choice(indices, size = split) # this line is to randomly select 10% of the data for validation\n",
    "        train_sampler = SubsetRandomSampler(list(self.selected_data))\n",
    "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "        self.train_loader = DataLoader(self.dataloader_train, batch_size=self.batch_size, sampler=train_sampler, num_workers=1)\n",
    "        self.validation_loader = DataLoader(self.dataloader_train, batch_size=self.batch_size, sampler=validation_sampler, num_workers=1)\n",
    "\n",
    "    def random_data(self, rounds):\n",
    "        if rounds == 1:    \n",
    "            # randomly select data\n",
    "            self.selected_data = set(range(self.seed_sample))\n",
    "            self.unexplored_data = self.unexplored_data.difference(self.selected_data)\n",
    "\n",
    "        else:\n",
    "            minimum_index = np.random.choice(list(self.unexplored_data), self.top_unc)\n",
    "            self.selected_data = self.selected_data.union(minimum_index)\n",
    "            self.unexplored_data = self.unexplored_data.difference(self.selected_data)\n",
    "\n",
    "\n",
    "    def activeDataSelection(self, rounds):\n",
    "        if rounds == 1:\n",
    "            self.selected_data = set(range(self.seed_sample))\n",
    "            self.unexplored_data = self.unexplored_data.difference(self.selected_data)\n",
    "        else:\n",
    "            all_data = DataLoader(self.dataloader_train, batch_size=self.batch_size, num_workers=1)\n",
    "            correct = 0\n",
    "            metrics = []\n",
    "            hook_handles = []\n",
    "            save_output = SaveOutput(self.instances, self.batch_size, self.rounds)\n",
    "            self.model.eval()\n",
    "            for layer in self.model.kl_layers:\n",
    "                hook_handles.append(layer.register_forward_hook(save_output))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_index, (X, y) in enumerate(all_data):\n",
    "                    batch_size = X.shape[0]\n",
    "                    save_output.batch_size = batch_size\n",
    "                    X = X.repeat(self.instances, 1)\n",
    "                    #y = y.squeeze()\n",
    "                    print(f'y: {y.shape}')\n",
    "                    y = y.repeat(self.instances)\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    y_pred = self.model(X)\n",
    "                    ensemble_outputs = y.reshape(self.instances, batch_size, 1) # reshape to instances x batch_size x 1 (output size)\n",
    "                    print(f'Ensemble outputs: {ensemble_outputs.shape}, {ensemble_outputs[0:5]}')\n",
    "                    print(f'output 1: ensemble_outputs[0, 0, :]: {ensemble_outputs[0, 0, :]}')\n",
    "                    print(f'output 2: ensemble_outputs[1, 0, :]: {ensemble_outputs[1, 0, :]}')\n",
    "                    print(f'output 3: ensemble_outputs[2, 0, :]: {ensemble_outputs[2, 0, :]}')\n",
    "\n",
    "                    entropy = self.get_entropy(ensemble_outputs)\n",
    "                    print(f'Entropy: {entropy.shape}, {entropy[0:5]}')\n",
    "                    metrics.append(entropy.cpu())\n",
    "\n",
    "                    save_output.clear()\n",
    "                    save_output.counter = 0\n",
    "                    for handle in hook_handles:\n",
    "                        handle.remove()\n",
    "\n",
    "                    metrics = torch.cat(metrics)\n",
    "                    new_indices = torch.argsort(metrics, descending=True).tolist()\n",
    "                    new_indices = [i for i in new_indices if i not in self.selected_data]\n",
    "\n",
    "                    self.selected_data.union(set(new_indices[:self.top_unc]))\n",
    "                    self.unexplored_data = self.unexplored_data.difference(self.selected_data)\n",
    "\n",
    "        \n",
    "\n",
    "    def TrainModel(self, rounds, epochs, is_validation):\n",
    "        print('running model')\n",
    "        t_total, v_total = 0, 0\n",
    "        t_r2_scores = []\n",
    "        if epochs == 1:\n",
    "            self.get_validation_data(is_validation)\n",
    "        self.model.train()\n",
    "        t_loss, v_loss = [], []\n",
    "        t_likelihood, v_likelihood = [], []\n",
    "        t_kl, v_kl = [], []\n",
    "        self.model.train()\n",
    "        m = len(self.train_loader)\n",
    "\n",
    "        print('before loop, this is the train loader: {}'.format(self.train_loader), len(self.train_loader))\n",
    "        for batch_index, (inputs, targets) in enumerate(self.train_loader):\n",
    "            print('running loop')\n",
    "            X = inputs.repeat(1, 1) # (number of mcmc samples, input size)\n",
    "            Y = targets.repeat(1, 1)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            outputs = self.model(X)\n",
    "            loss, log_likelihood, kl = self.objective(outputs, Y, self.model.kl_divergence(), 1 / m)\n",
    "            t_likelihood.append(log_likelihood.item())\n",
    "            t_kl.append(kl.item())\n",
    "            t_total += targets.size(0)\n",
    "          \n",
    "            # calculate r2 score manually\n",
    "            r2_score_value = 1 - (np.sum((outputs.detach().cpu().numpy() - targets.detach().cpu().numpy()) ** 2) / np.sum((targets.detach().cpu().numpy() - np.mean(targets.detach().cpu().numpy())) ** 2))\n",
    "            t_r2_scores.append(r2_score_value)\n",
    "            \n",
    "            t_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            # define the optimizer\n",
    "            optimizer = self.optimizer\n",
    "\n",
    "            optimizer.step()\n",
    "            for layer in self.model.kl_layers:\n",
    "                layer.clip_variances()\n",
    "        \n",
    "        if is_validation:\n",
    "            print(f'this is the validation data {self.validation_loader}, these are the characteristics {len(self.validation_loader)}')\n",
    "            m_val = len(self.validation_loader)\n",
    "            self.model.eval()\n",
    "            for batch_index, (inputs, targets) in enumerate(self.validation_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss_val, log_likelihood_val, kl_val = self.objective(outputs, targets, self.model.kl_divergence(), 1 / m_val)\n",
    "                v_total += targets.size(0)\n",
    "                v_loss.append(loss_val.item())\n",
    "                v_likelihood.append(log_likelihood_val.item())\n",
    "                v_kl.append(kl_val.item())\n",
    "\n",
    "            \n",
    "            avg_v_loss = np.average(v_loss)\n",
    "            avg_t_loss = np.average(t_loss)\n",
    "            avg_v_likelihood = np.average(v_likelihood)\n",
    "            avg_t_likelihood = np.average(t_likelihood)\n",
    "            avg_v_kl = np.average(v_kl)\n",
    "            avg_t_kl = np.average(t_kl)\n",
    "\n",
    "\n",
    "            print(\n",
    "                'epochs: {}, train loss: {}, train likelihood: {}, train kl: {}'.format(\n",
    "                    epochs, avg_t_loss, \\\n",
    "                    avg_t_likelihood, avg_t_kl))\n",
    "\n",
    "            print(\n",
    "                'epochs: {}, validation loss: {}, validation likelihood: {}, validation kl: {}'.format(\n",
    "                    epochs, avg_v_loss, \\\n",
    "                    avg_v_likelihood, avg_v_kl))\n",
    "\n",
    "            return avg_v_loss\n",
    "\n",
    "        else:\n",
    "            avg_t_loss = np.average(t_loss)\n",
    "            avg_t_likelihood = np.average(t_likelihood)\n",
    "            avg_t_kl = np.average(t_kl)\n",
    "            avg_t_r2 = np.average(t_r2_scores)\n",
    "\n",
    "            print(\n",
    "                'epochs: {}, train loss: {}, train likelihood: {}, train kl: {}, train_avg_R2: {}'.format(\n",
    "                    epochs, avg_t_loss, \\\n",
    "                    avg_t_likelihood, avg_t_kl, avg_t_r2))\n",
    "\n",
    "            return avg_t_loss, avg_t_r2\n",
    "\n",
    "    \n",
    "    def TestModel(self, rounds):\n",
    "        if device.type == 'cpu':\n",
    "            state = torch.load(self.train_weight_path, map_location=torch.device('cpu'))\n",
    "        else:\n",
    "            state = torch.load(self.train_weight_path)\n",
    "\n",
    "        self.model.load_state_dict(state['weights'])\n",
    "        print(f'Model loaded: {self.model}')\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        actual = []\n",
    "        mse_scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (inputs, targets) in enumerate(self.dataloader_test):\n",
    "                X, Y = inputs.to(device), targets.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                # Calculate the MSE loss for the batch\n",
    "                mse_loss = nn.MSELoss()\n",
    "                loss = mse_loss(outputs, Y)\n",
    "\n",
    "                # Get the MSE score as a Python scalar\n",
    "                mse_score = loss.item()\n",
    "                mse_scores.append(mse_score)\n",
    "\n",
    "                # Convert predictions and actual values to numpy arrays\n",
    "                predictions.append(outputs.detach().cpu().numpy())      \n",
    "                actual.append(Y.detach().cpu().numpy())\n",
    "                \n",
    "\n",
    "        predictions = np.concatenate(predictions)\n",
    "        actual = np.concatenate(actual)\n",
    "        df = pd.DataFrame(data = {'Predictions': predictions, 'Actual': actual})\n",
    "        df.loc['R2'] = 1 - np.sum((df.Actual - df.Predictions) ** 2) / np.sum((df.Actual - np.mean(df.Actual)) ** 2)\n",
    "        df.loc['MSE'] = mean_squared_error(df.Actual, df.Predictions)\n",
    "        \n",
    "        print('Non-Ensemble Test MSE:{:.3f}, TestR2:{:.3f}'.format(df.loc[\"MSE\"][0], df.loc[\"R2\"][0]))\n",
    "                \n",
    "\n",
    "\n",
    "    def getTrainedModel(self, rounds):\n",
    "        # path to save the trained model\n",
    "        self.train_weight_path = 'trainedModels/trained_weights/' + self.model_name + '_' + 'e' + str(self.epochs) + '_' + '-r' + str(rounds) + '-b' + str(self.batch_size) + '.pkl'\n",
    "        return (self.model, self.train_weight_path)\n",
    "\n",
    "\n",
    "    def saveModel(self, model, optimizer, path_to_save):\n",
    "        state = {\n",
    "            'rounds': self.rounds,\n",
    "            'weights': model.state_dict(),\n",
    "            'selected_data': self.selected_data,\n",
    "            'optimizer': self.optimizer.state_dict()\n",
    "            }\n",
    "\n",
    "        path_to_save = 'trainedModels/trained_weights/' + self.model_name + '_' + 'e' + str(self.epochs) + '_' + '-r' + str(self.rounds) + '-b' + str(self.batch_size) + '.pkl'\n",
    "\n",
    "        torch.save(state, path_to_save)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model\n",
      "before loop, this is the train loader: <torch.utils.data.dataloader.DataLoader object at 0x14c5981d0> 0\n",
      "epochs: 0, train loss: nan, train likelihood: nan, train kl: nan, train_avg_R2: nan\n",
      "running model\n",
      "before loop, this is the train loader: <torch.utils.data.dataloader.DataLoader object at 0x15037cc10> 1\n",
      "running loop\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 64, -1]' is invalid for input of size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m active_learning\u001b[39m.\u001b[39mrandom_data(rounds \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):  \u001b[39m# can be changed to epochs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m# if is_validation is False:\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     mse_loss, r2_score \u001b[39m=\u001b[39m active_learning\u001b[39m.\u001b[39;49mTrainModel(rounds \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, epochs \u001b[39m=\u001b[39;49m e, is_validation \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     16\u001b[0m     \u001b[39m# if is_validation is True:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39m#mse_loss = active_learning.TrainModel(rounds = 2, epochs = e, is_validation = True)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m     \u001b[39m# save the trained model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     active_learning\u001b[39m.\u001b[39msaveModel(model \u001b[39m=\u001b[39m active_learning\u001b[39m.\u001b[39mmodel, optimizer \u001b[39m=\u001b[39m active_learning\u001b[39m.\u001b[39moptimizer, path_to_save \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrainedModels/trained_weights\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 138\u001b[0m, in \u001b[0;36mrunActiveLearning.TrainModel\u001b[0;34m(self, rounds, epochs, is_validation)\u001b[0m\n\u001b[1;32m    136\u001b[0m Y \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    137\u001b[0m X, Y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 138\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(X)\n\u001b[1;32m    139\u001b[0m loss, log_likelihood, kl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective(outputs, Y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mkl_divergence(), \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m m)\n\u001b[1;32m    140\u001b[0m t_likelihood\u001b[39m.\u001b[39mappend(log_likelihood\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Models/simpleFFBNN.py:22\u001b[0m, in \u001b[0;36mSimpleFFBNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     23\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[1;32m     24\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args, result)\n\u001b[1;32m   1549\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[39m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mSaveOutput.__call__\u001b[0;34m(self, module, module_in, module_out)\u001b[0m\n\u001b[1;32m     12\u001b[0m sample_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[1;32m     13\u001b[0m \u001b[39m#outs = module_out.view(self.batch_size, -1)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m outs \u001b[39m=\u001b[39m module_out\u001b[39m.\u001b[39;49mview(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)[:, \u001b[39m0\u001b[39m, :]\n\u001b[1;32m     15\u001b[0m layer_size \u001b[39m=\u001b[39m outs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m write_summary\u001b[39m.\u001b[39mper_round_layer_output(layer_size, outs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrounds)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[3, 64, -1]' is invalid for input of size 100"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('trainedModels/trained_weights'):\n",
    "    os.makedirs('trainedModels/trained_weights')\n",
    "\n",
    "\n",
    "# use the class to run the active learning\n",
    "active_learning = runActiveLearning(model_name='simple', model=model, dataloader_train=dataset_train, top_unc = 10, dataloader_test=dataset_test, dataloader_val=dataset_val, epochs=10, rounds=2, learning_rate=0.001, batch_size=64, instances=3, seed_sample=4, retrain=False, resume_round=False, optimizer= torch.optim.Adam(model.parameters(), lr=0.001))\n",
    "\n",
    "active_learning.get_validation_data(is_validation = True)\n",
    "\n",
    "active_learning.random_data(rounds = 2)\n",
    "\n",
    "for e in range(5):  # can be changed to epochs\n",
    "    # if is_validation is False:\n",
    "    mse_loss, r2_score = active_learning.TrainModel(rounds = 2, epochs = e, is_validation = False)\n",
    "\n",
    "    # if is_validation is True:\n",
    "    #mse_loss = active_learning.TrainModel(rounds = 2, epochs = e, is_validation = True)\n",
    "\n",
    "    # save the trained model\n",
    "    active_learning.saveModel(model = active_learning.model, optimizer = active_learning.optimizer, path_to_save = 'trainedModels/trained_weights')\n",
    "    model, path = active_learning.getTrainedModel(rounds = 2)\n",
    "\n",
    "active_learning.TestModel(rounds = 2)\n",
    "\n",
    "active_learning.activeDataSelection(rounds = 2)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
