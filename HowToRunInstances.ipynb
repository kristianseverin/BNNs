{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from itertools import repeat\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utils import custom_data_loader, preprocess_data\n",
    "from Utils.SummaryWriter import LogSummary\n",
    "from Models.simpleFFBNN import SimpleFFBNN\n",
    "from Models.denseRegression import DenseRegressor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n",
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Function to get the device to be used for training the model\n",
    "    \"\"\"\n",
    "    cuda = torch.cuda.is_available()\n",
    "    print(\"CUDA Available: \", cuda)\n",
    "\n",
    "    if cuda:\n",
    "        gpu = GPUtil.getFirstAvailable()\n",
    "        print(\"GPU Available: \", gpu)\n",
    "        torch.cuda.set_device(gpu)\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(\"Device: \", device)\n",
    "    return device\n",
    "\n",
    "device = get_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Utils/config.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.X['savings'] = np.where(self.X['savings'] == 'low', 0, np.where(self.X['savings'] == 'medium', 1, 2))\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataloader_train, dataloader_test, dataloader_val = preprocess_data(pd.read_csv('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Data/quality_of_food.csv'), batch_size = 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFFBNN(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/trainedModels/simple_model.pth', map_location=torch.device('cpu'))\n",
    "#print(checkpoint)\n",
    "#model.load_state_dict(checkpoint['model'])\n",
    "model.load_state_dict(checkpoint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saveoutput():\n",
    "    def __init__(self, instances, batch_size, rounds):\n",
    "        self.T = instances\n",
    "        self.batch_size = batch_size\n",
    "        self.outputs = []\n",
    "        self.rounds = rounds\n",
    "        self.counter = 0\n",
    "\n",
    "\n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        if self.counter < 3:\n",
    "            sample_data = np.random.randint(self.batch_size)\n",
    "            outs = module_out.view(self.batch_size, -1)\n",
    "            #outs = module_out.view(self.T, self.batch_size, -1)[:, 0, :]\n",
    "            layer_size = outs.shape[1]\n",
    "\n",
    "            \n",
    "            write_summary.per_round_layer_output(layer_size, outs, self.rounds)\n",
    "\n",
    "            self.counter += 1\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_summary = LogSummary(name = 'Simple Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model:SimpleFFBNN(\n",
      "  (fc1): KlLayers (4 -> 10)\n",
      "  (fc2): KlLayers (10 -> 20)\n",
      "  (fc3): KlLayers (20 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class runActiveLearning():\n",
    "    def __init__(self, model_name, model, dataloader_train, dataloader_test, dataloader_val, epochs, rounds, learning_rate, batch_size, instances, highest_unc, seed_sample, retrain, resume_round):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_test = dataloader_test\n",
    "        self.dataloader_val = dataloader_val\n",
    "        self.epochs = epochs\n",
    "        self.rounds = rounds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.instances = instances\n",
    "        self.highest_unc = highest_unc\n",
    "        self.seed_sample = seed_sample\n",
    "\n",
    "\n",
    "        # a set of lists to store the selected indices with highest uncertainty\n",
    "        self.highest_unc = set([])\n",
    "        # unexplored data\n",
    "        self.unexplored_data = set(range(len(dataloader_train.dataset)))\n",
    "\n",
    "        if resume_round and not retrain:\n",
    "            self.InitModel(load_weigths=True, res_round=resume_round)\n",
    "        elif resume_round and retrain:\n",
    "            self.InitModel(load_weigths=False, res_round=resume_round)\n",
    "        else:\n",
    "            self.InitModel()\n",
    "\n",
    "\n",
    "        training_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print('Running Model:{}'.format(model, training_params, self.epochs, self.batch_size))\n",
    "\n",
    "\n",
    "    def InitModel(self,load_weigths = False, res_round = None):\n",
    "        if self.model_name == 'simple':\n",
    "            self.model = SimpleFFBNN(4, 1, )\n",
    "        else:\n",
    "            self.model = DenseRegressor(4, 1)\n",
    "            \n",
    "    def loadSeedModel(self):\n",
    "        # load the trained model\n",
    "        model = SimpleFFBNN(4, 1)\n",
    "        checkpoint = torch.load('Activelearning/SeedModels/simple_model_best.pth', map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint)\n",
    "        self.model = model\n",
    "        print(f'Model loaded: {model}')\n",
    "\n",
    "    def loadPreTrainedModel(self, load_weigths, res_round):\n",
    "        self.getTrainedModel(res_round)\n",
    "        # load weights\n",
    "        if DEVICE.type == 'cpu':\n",
    "            state = torch.load(self.train_weight_path, map_location=torch.device('cpu'))\n",
    "        else:\n",
    "            state = torch.load(self.train_weight_path)\n",
    "\n",
    "        self.highest_unc = state['highest_unc']\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        if load_weigths:\n",
    "            self.model.load_state_dict(state['weights'])\n",
    "            self.optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "    def getEntropy(self, y):\n",
    "        ensemble_probabilities = y.mean(0)\n",
    "       # print(f'Ensemble probabilities func: {ensemble_probabilities.shape}')\n",
    "        entropy = Categorical(probs = ensemble_probabilities).entropy()\n",
    "        # get the entropy of the ensemble probabilities\n",
    "        #entropy = -torch.sum(ensemble_probabilities * torch.log(ensemble_probabilities), dim=1)\n",
    "\n",
    "        #probabilities = Categorical(probs = ensemble_probabilities)\n",
    "\n",
    "       # print(f'Probabilities func: {probabilities.probs[0:5]}')\n",
    "\n",
    "        #entropy = probabilities.entropy()\n",
    "\n",
    "        #entropy  = -torch.sum(ensemble_probabilities * torch.log(ensemble_probabilities), dim=1)\n",
    "\n",
    "        #print(f'Entropy func: {entropy[0:5]}')\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def trainSeedModelClosedForm(self, rounds):\n",
    "        # loss function\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "        if rounds == 1:\n",
    "            self.highest_unc = set(range(self.seed_sample))\n",
    "            self.unexplored_data = self.unexplored.difference(self.highest_unc)\n",
    "\n",
    "\n",
    "\n",
    "        uncertainty = []\n",
    "        hook_handles = []\n",
    "        save_output = Saveoutput(instances= self.instances, batch_size=self.batch_size, rounds = self.rounds)\n",
    "        self.model.eval()\n",
    "        for layer in self.model.kl_layers:\n",
    "            hook_handles.append(layer.register_forward_hook(save_output))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (X, y) in enumerate(self.dataloader_train):\n",
    "                batch_size = X.shape[0]\n",
    "                #print(f'X before repeat: {X}')\n",
    "               # print(f'y before repeat: {y}')\n",
    "                save_output.batch_size = batch_size\n",
    "                X = X.repeat(self.instances, 1)\n",
    "                #print(f'X after repeat: {X}, {X.shape}')\n",
    "                \n",
    "\n",
    "                y = y.squeeze()\n",
    "                # squeeze the y to remove the extra dimension\n",
    "                \n",
    "                y = y.repeat(self.instances)\n",
    "        \n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_pred = model(X)\n",
    "                print(f'y_pred: {y_pred.shape}, {y_pred[0:5]}, {y_pred[128:133]}')\n",
    "                \n",
    "                \n",
    "\n",
    "                ensemble_outputs = y.reshape(self.instances, batch_size, 1) # reshape to instances x batch_size x 1 (output size)\n",
    "                \n",
    "                \n",
    "                uncertainty.append(self.getEntropy(ensemble_outputs))\n",
    "                   \n",
    "        \n",
    "            save_output.clear()\n",
    "            save_output.counter = 0\n",
    "            for handle in hook_handles:\n",
    "                handle.remove()\n",
    "            \n",
    "            uncertainty = torch.cat(uncertainty)\n",
    "            \n",
    "            new_indices = torch.argsort(uncertainty, descending=True).tolist()\n",
    "\n",
    "            # remove already selected data\n",
    "            new_indices = [i for i in new_indices if i not in self.highest_unc]  \n",
    "\n",
    "            # select the highest uncertainty data\n",
    "            self.highest_unc.union(set(new_indices[:self.seed_sample]))\n",
    "            print(f'higheset_unc: {self.highest_unc}')\n",
    "            self.unexplored_data = self.unexplored_data.difference(self.highest_unc)\n",
    "\n",
    "    \n",
    "    def objective(self, output, target, kl, beta):\n",
    "        loss_fun = nn.MSELoss()\n",
    "        discrimination_error = loss_fun(output.view(-1), target.view(-1))\n",
    "        variational_bound = discrimination_error + beta * kl\n",
    "        return variational_bound, discrimination_error, kl\n",
    "\n",
    "\n",
    "    def getTrainedModel(self, res_round):\n",
    "        retrain = self.retrain\n",
    "        self.train_weight_path = f'Activelearning/SeedModels/simple_model_best.pth'\n",
    "        if res_round:\n",
    "            self.train_weight_path = f'Activelearning/SeedModels/simple_model_best.pth'\n",
    "        else:\n",
    "            self.train_weight_path = f'Activelearning/SeedModels/simple_model_best.pth'\n",
    "\n",
    "        return (self.model, self.train_weight_path)\n",
    "\n",
    "\n",
    "\n",
    "    def trainModel(self, rounds, epochs):\n",
    "        t_total, v_total = 0, 0\n",
    "        t_r2_scores, v_r2_scores = [], []\n",
    "        self.model.train()\n",
    "        t_loss, v_loss = [], []\n",
    "        t_likelihood, v_likelihood = [], []\n",
    "        t_kl, v_kl = [], []\n",
    "        self.model.train()\n",
    "        m = len(self.dataloader_train)\n",
    "\n",
    "        for batch_index, (inputs, targets) in enumerate(self.dataloader_train):\n",
    "            X = inputs.repeat(1, 1)  # (number of mcmc samples)\n",
    "            Y = targets.repeat(1)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            outputs = self.model(X)\n",
    "            loss, log_likelihood, kl = self.objective(outputs, Y, self.model.kl_divergence, m)\n",
    "        \n",
    "        t_total += target.size(0)\n",
    "        t_r2_scores.append(r2_score(outputs.detach().cpu()[:,0], targets.numpy()))\n",
    "        t_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for layer in self.model.kl_layers:\n",
    "            layer.clip_variances()\n",
    "\n",
    "        \n",
    "        # validate\n",
    "        m_val = len(self.dataloader_val)\n",
    "        self.model.eval()\n",
    "        for batch_index, (inputs, targets) in enumerate(self.dataloader_val):\n",
    "            X = self.model(inputs)\n",
    "            loss_val, log_likelihood_val, kl_val = self.objective(outputs, Y, self.model.kl_divergence, m_val)\n",
    "        \n",
    "            v_total += target.size(0)\n",
    "            v_loss.append(loss_val.item())\n",
    "            v_likelihood.append(log_likelihood_val.item())\n",
    "            v_kl.append(kl_val.item())\n",
    "           # v_r2_scores.append(r2_score(outputs.detach().cpu()[:,0], targets.numpy()))\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "   # write_summary = LogSummary(name='Simple Model')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use the class to run the active learning\n",
    "active_learning = runActiveLearning(model_name='simple', model=model, dataloader_train=dataloader_train, dataloader_test=dataloader_test, dataloader_val=dataloader_val, epochs=100, rounds=2, learning_rate=0.001, batch_size=128, instances=3, highest_unc=6, seed_sample=4, retrain=False, resume_round=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSeedModelClosedForm(self):\n",
    "    hook_handles = []\n",
    "    save_output = saveOutput(self.instances, self.batch_size, 0)\n",
    "    self.model.eval()\n",
    "    for layer in self.model.layers:\n",
    "        hook_handles.append(layer.register_forward_hook(save_output))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_index, (X, y) in enumerate(self.data_train):\n",
    "            batch_size = X.shape[0]\n",
    "            save_output.batch_size = batch_size\n",
    "            X = X.repeat(self.instances, 1)\n",
    "            y = y.repeat(self.instances)\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            ensemble_outputs = self.model(X)\n",
    "\n",
    "        save_output.clear()\n",
    "        save_output.counter = 0\n",
    "        for handle in hook_handles:\n",
    "            handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Utils/config.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.X['savings'] = np.where(self.X['savings'] == 'low', 0, np.where(self.X['savings'] == 'medium', 1, 2))\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataloader_train, dataloader_test, dataloader_val, dataset_train, dataset_test, dataset_val = preprocess_data(pd.read_csv('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Data/quality_of_food.csv'), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class runActiveLearning():\n",
    "    def __init__(self, model_name, model, dataloader_train, dataloader_test, dataloader_val, epochs, rounds, learning_rate, \n",
    "    batch_size, instances, seed_sample, retrain, resume_round, optimizer):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_test = dataloader_test\n",
    "        self.dataloader_val = dataloader_val\n",
    "        self.epochs = epochs\n",
    "        self.rounds = rounds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.instances = instances\n",
    "        self.seed_sample = seed_sample\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # a set of lists to store the selected indices with highest uncertainty\n",
    "        self.selected_data = set([])\n",
    "        # unexplored data\n",
    "        self.unexplored_data = set(range(len(dataloader_train)))\n",
    "\n",
    "        # make sure sklearn.metrics.r2_score is imported\n",
    "        #self.r2_score = r2_score\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def objective(self, output, target, kl, beta):\n",
    "        '''Objective function to calculate the loss function / KL divergence'''\n",
    "        loss_fun = nn.MSELoss()\n",
    "        discrimination_error = loss_fun(output.view(-1), target)\n",
    "        variational_bound = discrimination_error + beta * kl\n",
    "        return variational_bound, discrimination_error, kl\n",
    "\n",
    "\n",
    "\n",
    "    def get_validation_data(self, is_validation):\n",
    "        if not is_validation:\n",
    "            # train sampler randomly samples data from the selected data set\n",
    "            train_sampler = SubsetRandomSampler(list(self.selected_data))\n",
    "            # train loader will load the data from the train sampler\n",
    "            self.train_loader = DataLoader(self.dataloader_train, batch_size=self.batch_size, sampler=train_sampler, num_workers=1)\n",
    "\n",
    "        indices = list(self.unexplored_data)\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(0.1 * len(indices)))  # this line is to split the data into 90% training and 10% validation\n",
    "        validation_idx = np.random.choice(indices, size = split) # this line is to randomly select 10% of the data for validation\n",
    "        train_sampler = SubsetRandomSampler(list(self.selected_data))\n",
    "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "        self.train_loader = DataLoader(self.dataloader_train, batch_size=self.batch_size, sampler=train_sampler, num_workers=1)\n",
    "        self.validation_loader = DataLoader(self.dataloader_train, batch_size=self.batch_size, sampler=validation_sampler, num_workers=1)\n",
    "\n",
    "    def random_data(self, rounds):\n",
    "        # randomly select data\n",
    "        self.selected_data = set(range(self.seed_sample))\n",
    "        self.unexplored_data = self.unexplored_data.difference(self.selected_data)\n",
    "        \n",
    "\n",
    "    def TrainModel(self, rounds, epochs, is_validation):\n",
    "        print('running model')\n",
    "        t_total, v_total = 0, 0\n",
    "        t_r2_scores = []\n",
    "        if epochs == 1:\n",
    "            self.get_validation_data(is_validation)\n",
    "        self.model.train()\n",
    "        t_loss, v_loss = [], []\n",
    "        t_likelihood, v_likelihood = [], []\n",
    "        t_kl, v_kl = [], []\n",
    "        self.model.train()\n",
    "        m = len(self.train_loader)\n",
    "\n",
    "        print('before loop, this is the train loader: {}'.format(self.train_loader), len(self.train_loader))\n",
    "        for batch_index, (inputs, targets) in enumerate(self.train_loader):\n",
    "            print('running loop')\n",
    "            X = inputs.repeat(1, 1) # (number of mcmc samples, input size)\n",
    "            Y = targets.repeat(1, 1)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            outputs = self.model(X)\n",
    "            loss, log_likelihood, kl = self.objective(outputs, Y, self.model.kl_divergence(), 1 / m)\n",
    "            t_likelihood.append(log_likelihood.item())\n",
    "            t_kl.append(kl.item())\n",
    "            t_total += targets.size(0)\n",
    "          \n",
    "            # calculate r2 score manually\n",
    "            r2_score_value = 1 - (np.sum((outputs.detach().cpu().numpy() - targets.detach().cpu().numpy()) ** 2) / np.sum((targets.detach().cpu().numpy() - np.mean(targets.detach().cpu().numpy())) ** 2))\n",
    "            t_r2_scores.append(r2_score_value)\n",
    "            \n",
    "            t_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            # define the optimizer\n",
    "            optimizer = self.optimizer\n",
    "\n",
    "            optimizer.step()\n",
    "            for layer in self.model.kl_layers:\n",
    "                layer.clip_variances()\n",
    "        \n",
    "        if is_validation:\n",
    "            print(f'this is the validation data {self.validation_loader}, these are the characteristics {len(self.validation_loader)}')\n",
    "            m_val = len(self.validation_loader)\n",
    "            self.model.eval()\n",
    "            for batch_index, (inputs, targets) in enumerate(self.validation_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss_val, log_likelihood_val, kl_val = self.objective(outputs, targets, self.model.kl_divergence(), 1 / m_val)\n",
    "                v_total += targets.size(0)\n",
    "                v_loss.append(loss_val.item())\n",
    "                v_likelihood.append(log_likelihood_val.item())\n",
    "                v_kl.append(kl_val.item())\n",
    "\n",
    "            \n",
    "            avg_v_loss = np.average(v_loss)\n",
    "            avg_t_loss = np.average(t_loss)\n",
    "            avg_v_likelihood = np.average(v_likelihood)\n",
    "            avg_t_likelihood = np.average(t_likelihood)\n",
    "            avg_v_kl = np.average(v_kl)\n",
    "            avg_t_kl = np.average(t_kl)\n",
    "\n",
    "\n",
    "            print(\n",
    "                'epochs: {}, train loss: {}, train likelihood: {}, train kl: {}'.format(\n",
    "                    epochs, avg_t_loss, \\\n",
    "                    avg_t_likelihood, avg_t_kl))\n",
    "\n",
    "            print(\n",
    "                'epochs: {}, validation loss: {}, validation likelihood: {}, validation kl: {}'.format(\n",
    "                    epochs, avg_v_loss, \\\n",
    "                    avg_v_likelihood, avg_v_kl))\n",
    "\n",
    "            return avg_v_loss\n",
    "\n",
    "        else:\n",
    "            avg_t_loss = np.average(t_loss)\n",
    "            avg_t_likelihood = np.average(t_likelihood)\n",
    "            avg_t_kl = np.average(t_kl)\n",
    "            avg_t_r2 = np.average(t_r2_scores)\n",
    "\n",
    "            print(\n",
    "                'epochs: {}, train loss: {}, train likelihood: {}, train kl: {}, train_avg_R2: {}'.format(\n",
    "                    epochs, avg_t_loss, \\\n",
    "                    avg_t_likelihood, avg_t_kl, avg_t_r2))\n",
    "\n",
    "            return avg_t_loss, avg_t_r2\n",
    "\n",
    "    \n",
    "    def TestModel(self, rounds):\n",
    "        if device.type == 'cpu':\n",
    "            state = torch.load(self.train_weight_path, map_location=torch.device('cpu'))\n",
    "        else:\n",
    "            state = torch.load(self.train_weight_path)\n",
    "\n",
    "        self.model.load_state_dict(state['weights'])\n",
    "        print(f'Model loaded: {self.model}')\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        actual = []\n",
    "        mse_scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (inputs, targets) in enumerate(self.dataloader_test):\n",
    "                X, Y = inputs.to(device), targets.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                X = X.detach().cpu().numpy()\n",
    "                predictions.append(X)\n",
    "               \n",
    "                actual.append(Y.detach().cpu().numpy())\n",
    "                \n",
    "                mse_scores.append(nn.MSELoss(outputs, targets).item())\n",
    "        print(f'MSE scores: {mse_scores}')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def getTrainedModel(self, rounds):\n",
    "        # path to save the trained model\n",
    "        self.train_weight_path = 'trainedModels/trained_weights/' + self.model_name + '_' + 'e' + str(self.epochs) + '_' + '-r' + str(rounds) + '-b' + str(self.batch_size) + '.pkl'\n",
    "        return (self.model, self.train_weight_path)\n",
    "\n",
    "\n",
    "    def saveModel(self, model, optimizer, path_to_save):\n",
    "        state = {\n",
    "            'rounds': self.rounds,\n",
    "            'weights': model.state_dict(),\n",
    "            'selected_data': self.selected_data,\n",
    "            'optimizer': self.optimizer.state_dict()\n",
    "            }\n",
    "\n",
    "        path_to_save = 'trainedModels/trained_weights/' + self.model_name + '_' + 'e' + str(self.epochs) + '_' + '-r' + str(self.rounds) + '-b' + str(self.batch_size) + '.pkl'\n",
    "\n",
    "        torch.save(state, path_to_save)\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model\n",
      "before loop, this is the train loader: <torch.utils.data.dataloader.DataLoader object at 0x15b4fe3d0> 0\n",
      "epochs: 0, train loss: nan, train likelihood: nan, train kl: nan, train_avg_R2: nan\n",
      "running model\n",
      "before loop, this is the train loader: <torch.utils.data.dataloader.DataLoader object at 0x15b4fcc10> 1\n",
      "running loop\n",
      "epochs: 1, train loss: 1317.9713134765625, train likelihood: 1.2902977466583252, train kl: 1316.6810302734375, train_avg_R2: -0.01632559299468994\n",
      "running model\n",
      "before loop, this is the train loader: <torch.utils.data.dataloader.DataLoader object at 0x15b4fcc10> 1\n",
      "running loop\n",
      "epochs: 2, train loss: 1317.732666015625, train likelihood: 1.2904441356658936, train kl: 1316.4422607421875, train_avg_R2: -0.016655325889587402\n",
      "running model\n",
      "before loop, this is the train loader: <torch.utils.data.dataloader.DataLoader object at 0x15b4fcc10> 1\n",
      "running loop\n",
      "epochs: 3, train loss: 1317.4940185546875, train likelihood: 1.2906041145324707, train kl: 1316.203369140625, train_avg_R2: -0.01697540283203125\n",
      "running model\n",
      "before loop, this is the train loader: <torch.utils.data.dataloader.DataLoader object at 0x15b4fcc10> 1\n",
      "running loop\n",
      "epochs: 4, train loss: 1317.255615234375, train likelihood: 1.2907514572143555, train kl: 1315.96484375, train_avg_R2: -0.017270207405090332\n",
      "Model loaded: SimpleFFBNN(\n",
      "  (fc1): KlLayers (4 -> 10)\n",
      "  (fc2): KlLayers (10 -> 20)\n",
      "  (fc3): KlLayers (20 -> 1)\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MSELoss' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     active_learning\u001b[39m.\u001b[39msaveModel(model \u001b[39m=\u001b[39m active_learning\u001b[39m.\u001b[39mmodel, optimizer \u001b[39m=\u001b[39m active_learning\u001b[39m.\u001b[39moptimizer, path_to_save \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrainedModels/trained_weights\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m     model, path \u001b[39m=\u001b[39m active_learning\u001b[39m.\u001b[39mgetTrainedModel(rounds \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m active_learning\u001b[39m.\u001b[39;49mTestModel(rounds \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[105], line 169\u001b[0m, in \u001b[0;36mrunActiveLearning.TestModel\u001b[0;34m(self, rounds)\u001b[0m\n\u001b[1;32m    165\u001b[0m         predictions\u001b[39m.\u001b[39mappend(X)\n\u001b[1;32m    167\u001b[0m         actual\u001b[39m.\u001b[39mappend(Y\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m--> 169\u001b[0m         mse_scores\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39;49mMSELoss(outputs, targets)\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m    170\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMSE scores: \u001b[39m\u001b[39m{\u001b[39;00mmse_scores\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MSELoss' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('trainedModels/trained_weights'):\n",
    "    os.makedirs('trainedModels/trained_weights')\n",
    "\n",
    "\n",
    "# use the class to run the active learning\n",
    "active_learning = runActiveLearning(model_name='simple', model=model, dataloader_train=dataset_train, dataloader_test=dataset_test, dataloader_val=dataset_val, epochs=100, rounds=2, learning_rate=0.001, batch_size=128, instances=3, seed_sample=4, retrain=False, resume_round=False, optimizer= torch.optim.Adam(model.parameters(), lr=0.001))\n",
    "\n",
    "active_learning.get_validation_data(is_validation = True)\n",
    "\n",
    "active_learning.random_data(rounds = 2)\n",
    "\n",
    "for e in range(5):\n",
    "    # if is_validation is False:\n",
    "    mse_loss, r2_score = active_learning.TrainModel(rounds = 2, epochs = e, is_validation = False)\n",
    "\n",
    "    # if is_validation is True:\n",
    "    #mse_loss = active_learning.TrainModel(rounds = 2, epochs = e, is_validation = True)\n",
    "\n",
    "    # save the trained model\n",
    "    active_learning.saveModel(model = active_learning.model, optimizer = active_learning.optimizer, path_to_save = 'trainedModels/trained_weights')\n",
    "    model, path = active_learning.getTrainedModel(rounds = 2)\n",
    "\n",
    "active_learning.TestModel(rounds = 2)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
