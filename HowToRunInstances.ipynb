{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from itertools import repeat\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utils import custom_data_loader, preprocess_data, preprocess_activeL_data\n",
    "from Utils.SummaryWriter import LogSummary\n",
    "from Models.simpleFFBNN import SimpleFFBNN\n",
    "from Models.denseRegression import DenseRegressor\n",
    "from Models.paperModel import SimpleFFBNNPaper\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n",
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Function to get the device to be used for training the model\n",
    "    \"\"\"\n",
    "    cuda = torch.cuda.is_available()\n",
    "    print(\"CUDA Available: \", cuda)\n",
    "\n",
    "    if cuda:\n",
    "        gpu = GPUtil.getFirstAvailable()\n",
    "        print(\"GPU Available: \", gpu)\n",
    "        torch.cuda.set_device(gpu)\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(\"Device: \", device)\n",
    "    return device\n",
    "\n",
    "device = get_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "#dataloader_train, dataloader_test, dataloader_val = preprocess_data(pd.read_csv('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Data/quality_of_food.csv'), batch_size = 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFFBNNPaper(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = torch.load('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/trainedModels/simple_model.pth', map_location=torch.device('cpu'))\n",
    "#print(checkpoint)\n",
    "#model.load_state_dict(checkpoint['model'])\n",
    "#model.load_state_dict(checkpoint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput():\n",
    "    def __init__(self, instances, batch_size, rounds):\n",
    "        self.T = instances\n",
    "        self.batch_size = batch_size\n",
    "        self.outputs = []\n",
    "        self.rounds = rounds\n",
    "        self.counter = 0\n",
    "\n",
    "\n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        if self.counter < 3:\n",
    "            sample_data = np.random.randint(self.batch_size)\n",
    "            #outs = module_out.view(self.batch_size, -1)\n",
    "            outs = module_out.view(self.T, self.batch_size, -1)[:, 0, :]\n",
    "            layer_size = outs.shape[1]\n",
    "\n",
    "            \n",
    "            write_summary.per_round_layer_output(layer_size, outs, self.rounds)\n",
    "            \n",
    "            # print the output of the layer\n",
    "            \n",
    "            self.counter += 1\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Utils.config.custom_data_loader object at 0x146cb1750>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Utils/config.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.X['savings'] = np.where(self.X['savings'] == 'low', 0, np.where(self.X['savings'] == 'medium', 1, 2))\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_test, dataset_activeL, df_custom = preprocess_activeL_data(pd.read_csv('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Data/quality_of_food.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utils.config.custom_data_loader"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using the old function\n",
    "#dataloader_train, dataloader_test, dataloader_val, dataset_train, dataset_test, dataset_val = preprocess_data(pd.read_csv('/Users/kristian/Documents/Skole/9. Semester/Thesis Preparation/Code/BNNs/Data/quality_of_food.csv'), batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class runActiveLearning():\n",
    "    def __init__(self, model_name, model, top_unc, dataloader_train, dataloader_test, dataset_active_l, epochs, rounds, learning_rate, \n",
    "    batch_size, instances, seed_sample, retrain, resume_round, optimizer, df_custom):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.top_unc = top_unc\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_test = dataloader_test\n",
    "        self.dataset_active_l = dataset_active_l\n",
    "        self.epochs = epochs\n",
    "        self.rounds = rounds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.instances = instances\n",
    "        self.seed_sample = seed_sample\n",
    "        self.optimizer = optimizer\n",
    "        self.df_custom = df_custom\n",
    "        \n",
    "\n",
    "        # a set of lists to store the selected indices with highest uncertainty\n",
    "        self.selected_data = set([])\n",
    "        # unexplored data\n",
    "        self.unexplored_data = set(range(len(dataloader_train)))\n",
    "\n",
    "        # make sure sklearn.metrics.r2_score is imported\n",
    "        #self.r2_score = r2_score\n",
    "\n",
    "\n",
    "    \n",
    "    def objective(self, output, target, kl, beta):\n",
    "        '''Objective function to calculate the loss function / KL divergence'''\n",
    "        loss_fun = nn.MSELoss()\n",
    "        discrimination_error = loss_fun(output.view(-1), target)\n",
    "        variational_bound = discrimination_error + beta * kl\n",
    "        return variational_bound, discrimination_error, kl\n",
    "\n",
    "    def get_entropy(self, y):\n",
    "        '''Function to calculate the entropy of the ensemble outputs\n",
    "        y: the ensemble outputs (shape: 30, 64, 1)'''\n",
    "        y = y.squeeze()\n",
    "        y = y.view(self.instances, -1)\n",
    "        y = y.permute(1, 0)\n",
    "        y = F.softmax(y, dim=1)\n",
    "        _entropy = -torch.sum(y * torch.log(y + 1e-8), dim=1)\n",
    "        # H = entropy(y, axis=1) # calculate the entropy of the ensemble outputs using scipy.stats.entropy\n",
    "        ''' H and _entropy are very similar, but the _entropy is calculated using pytorch \n",
    "        and the H is calculated using scipy.stats.entropy. The _entropy is used in the code,\n",
    "        but the H is kept in the comments for reference.'''\n",
    "        return _entropy\n",
    "\n",
    "    def get_validation_data(self, is_validation):\n",
    "        if not is_validation:\n",
    "            # train sampler randomly samples data from the selected data set\n",
    "            train_sampler = SubsetRandomSampler(list(self.selected_data))\n",
    "            # train loader will load the data from the train sampler\n",
    "            self.train_loader = DataLoader(self.dataloader_train, batch_size=self.batch_size, sampler=train_sampler, num_workers=1)\n",
    "\n",
    "        indices = list(self.unexplored_data)\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(0.1 * len(indices)))  # this line is to split the training_data into 90% training and 10% validation\n",
    "        validation_idx = np.random.choice(indices, size = split) # this line is to randomly select 10% of the data for validation\n",
    "        train_sampler = SubsetRandomSampler(list(self.selected_data))\n",
    "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "        self.train_loader = DataLoader(self.dataloader_train, batch_size=self.batch_size, sampler=train_sampler, num_workers=1)\n",
    "        self.validation_loader = DataLoader(self.dataloader_train, batch_size=self.batch_size, sampler=validation_sampler, num_workers=1)\n",
    "\n",
    "    def random_data(self, rounds):\n",
    "        if rounds == 0:    \n",
    "            # randomly select data\n",
    "            self.selected_data = set(range(self.dataloader_train))  # seed sample in Rakeesh & Jain paper\n",
    "           \n",
    "            #self.unexplored_data = self.unexplored_data.difference(self.selected_data) # all \n",
    "\n",
    "        else:\n",
    "            minimum_index = np.random.choice(list(self.unexplored_data), self.top_unc)\n",
    "      \n",
    "            self.selected_data = self.selected_data.union(minimum_index)\n",
    "\n",
    "            self.unexplored_data = self.unexplored_data.difference(self.selected_data)\n",
    "\n",
    "\n",
    "\n",
    "    def activeDataSelection(self, rounds):\n",
    "        \n",
    "        # for the first round select all the data\n",
    "        if rounds == 1:\n",
    "            # select all the active data as all data should be predicted with uncertainty\n",
    "            #self.active_data = dataset_activeL\n",
    "            self.all_data = DataLoader(self.dataset_active_l, batch_size=self.batch_size, num_workers=1)\n",
    "            correct = 0\n",
    "            metrics = []\n",
    "            hook_handles = []\n",
    "            save_output = SaveOutput(self.instances, self.batch_size, self.rounds)\n",
    "            self.model.eval()\n",
    "            for layer in self.model.kl_layers:\n",
    "                handle = layer.register_forward_hook(save_output)\n",
    "                hook_handles.append(handle)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_index, (X, y) in enumerate(self.all_data):\n",
    "                    batch_size = X.shape[0]\n",
    "                    save_output.batch_size = batch_size\n",
    "                    X = X.repeat(self.instances, 1)\n",
    "                    y = y.squeeze()\n",
    "                    y = y.repeat(self.instances)\n",
    "                    \n",
    "                    X, y = X.to(device), y.to(device)\n",
    "\n",
    "                    y_pred = self.model(X)\n",
    "                   # print(f'y_pred: {y_pred.shape}, {y_pred[0:5]}')\n",
    "\n",
    "\n",
    "                    ensemble_outputs = y_pred.reshape(self.instances, batch_size, 1)\n",
    "          \n",
    "                    entropy = self.get_entropy(ensemble_outputs)\n",
    "        \n",
    "                    metrics.append(entropy)\n",
    "\n",
    "                save_output.clear()\n",
    "                save_output.counter = 0\n",
    "                for handle in hook_handles:\n",
    "                    handle.remove()\n",
    "\n",
    "                metrics = torch.cat(metrics)\n",
    "                \n",
    "                # print all the uniwue values in the metrics\n",
    "\n",
    "                \n",
    "                new_indices = torch.argsort(metrics, descending=True).tolist()\n",
    "\n",
    "           \n",
    "                self.selected_data = set(new_indices[:self.top_unc])\n",
    "      \n",
    "                self.unexplored_data = self.unexplored_data.difference(self.selected_data)\n",
    "        \n",
    "                self.non_labelled_data = self.selected_data\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "           \n",
    "         \n",
    "\n",
    "    def annotateSelectedData(self, rounds):\n",
    "        if rounds == 1:\n",
    "            # do nothing\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            indices = list(self.non_labelled_data)\n",
    "            data_to_annotate = [self.dataset_active_l[i] for i in indices]\n",
    "            \n",
    "            def refit_and_rescale(data):\n",
    "                \n",
    "                data_to_fit_X = self.df_custom.X\n",
    "                data_to_fit_y = self.df_custom.y\n",
    "\n",
    "                scaler = StandardScaler().fit(data_to_fit_X)\n",
    "                # get the x_values from the data to be annotated\n",
    "                \n",
    "                x_values = [x for x, y in data]\n",
    "                y_values = [y for x, y in data]\n",
    "\n",
    "                x_arrays =[x.numpy() for x in x_values]\n",
    "                y_arrays = [y.numpy() for y in y_values]\n",
    "                \n",
    "                x_descaled = [torch.tensor(scaler.inverse_transform(x.reshape(1, -1))) for x in x_arrays]\n",
    "\n",
    "                # get the x_values in numpy format\n",
    "                x_np = [x.numpy() for x in x_descaled]\n",
    "                x_flattened = [arr.flatten() for arr in x_np]\n",
    "\n",
    "                # create a dataframe from the x_values\n",
    "                x_df = pd.DataFrame(x_flattened, columns = ['income', 'time', 'savings', 'guests'])\n",
    "\n",
    "                '''due to the way the data is transformed and inverse transformed 0 value are not exactly 0, but very close to 0.\n",
    "                Therefore, the values close to 0 are replaced with 0'''\n",
    "                tolerance = 1e-5\n",
    "                x_df = x_df.mask(x_df.abs() < tolerance, 0)\n",
    "\n",
    "                return x_df\n",
    "\n",
    "            \n",
    "            df = refit_and_rescale(data_to_annotate)\n",
    "\n",
    "\n",
    "            def determine_quality(data):\n",
    "\n",
    "                '''quality of food is determined by the income, time, savings and guests \n",
    "                following the approach the data was originally generated with.\n",
    "                This function is therefore acting as the oracle.\n",
    "                args:\n",
    "                data: the data to be annotated in a pandas dataframe format with the columns income, time, savings and guests\n",
    "                \n",
    "                nb: the y_values should be generated without random noise as noise doesn't make theoretical sense in this context'''\n",
    "                quality_based_on_income = np.where(data['income'] >= 7000, 5,\n",
    "                np.where(data['income'] >= 4000, 4,\n",
    "                np.where(data['income'] >= 3000, 3,\n",
    "                np.where(data['income'] >= 2000, 2, 1))))\n",
    "                quality_based_on_time = np.where(data['time'] >= 16, 1, 5)\n",
    "                quality_based_on_savings = np.where(data['savings'] == 2, 5,\n",
    "                np.where(data['savings'] == 1, 3, 1))\n",
    "                quality_based_on_guests = np.where(data['guests'] == 0, 3, 5) \n",
    "                quality_of_food = (quality_based_on_income * 0.4 + quality_based_on_time * 0.1 + quality_based_on_savings * 0.2 + quality_based_on_guests * 0.3) / 1\n",
    "\n",
    "                # make the quality of food y in the data\n",
    "                data['quality_of_food'] = quality_of_food\n",
    "\n",
    "                return data\n",
    "\n",
    "              \n",
    "            annotated_data = determine_quality(df)\n",
    "            \n",
    "            # scale the newly annotated data\n",
    "            x_scaler = StandardScaler().fit(self.df_custom.X)\n",
    "            x = annotated_data.drop('quality_of_food', axis = 1)\n",
    "            y = annotated_data['quality_of_food']\n",
    "            x_scaled = x_scaler.transform(x)\n",
    "            x_scaled = torch.tensor(x_scaled)\n",
    "            y_scaler = StandardScaler().fit(self.df_custom.y.reshape(-1, 1))\n",
    "            y_scaled = y_scaler.transform(y.values.reshape(-1, 1))\n",
    "            y_scaled = torch.tensor(y_scaled)\n",
    "\n",
    "            # create a tensor dataset from the annotated data\n",
    "            dataset = torch.utils.data.TensorDataset(x_scaled, y_scaled)\n",
    "            combined_dataset = torch.utils.data.ConcatDataset([self.train_loader.dataset, dataset])\n",
    "            print(f'Length of the combined dataset: {len(combined_dataset)}, {combined_dataset}')\n",
    "            self.train_loader = DataLoader(combined_dataset, batch_size=self.batch_size, num_workers=1)\n",
    "            print(f'Length of the train loader: {len(self.train_loader)}')\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    def TrainModel(self, rounds, epochs, is_validation):\n",
    "    \n",
    "        \n",
    "        #print('running model')\n",
    "        t_total, v_total = 0, 0\n",
    "        t_r2_scores = []\n",
    "        if epochs == 1:\n",
    "            self.get_validation_data(is_validation)\n",
    "        self.model.train()\n",
    "        t_loss, v_loss = [], []\n",
    "        t_likelihood, v_likelihood = [], []\n",
    "        t_kl, v_kl = [], []\n",
    "        self.model.train()\n",
    "        m = len(self.train_loader)\n",
    "       # print(f'this is the train loader: {self.train_loader}, {len(self.train_loader)}')\n",
    "\n",
    "       # print('before loop, this is the train loader: {}'.format(self.train_loader), len(self.train_loader))\n",
    "        for batch_index, (inputs, targets) in enumerate(self.train_loader):\n",
    "         #   print('running loop')\n",
    "            X = inputs.repeat(self.instances, 1) # (number of mcmc samples, input size)\n",
    "            Y = Y.repeat(self.instances) # (number of mcmc samples, output size)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            outputs = self.model(X)\n",
    "            loss, log_likelihood, kl = self.objective(outputs, Y, self.model.kl_divergence(), 1 / m)\n",
    "            t_likelihood.append(log_likelihood.item())\n",
    "            t_kl.append(kl.item())\n",
    "            t_total += targets.size(0)\n",
    "          \n",
    "            # calculate r2 score manually\n",
    "            r2_score_value = 1 - (np.sum((outputs.detach().cpu().numpy() - targets.detach().cpu().numpy()) ** 2) / np.sum((targets.detach().cpu().numpy() - np.mean(targets.detach().cpu().numpy())) ** 2))\n",
    "            t_r2_scores.append(r2_score_value)\n",
    "            \n",
    "            t_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            # define the optimizer\n",
    "            optimizer = self.optimizer\n",
    "\n",
    "            optimizer.step()\n",
    "            for layer in self.model.kl_layers:\n",
    "                layer.clip_variances()\n",
    "        \n",
    "        if is_validation:\n",
    "            #print(f'this is the validation data {self.validation_loader}, these are the characteristics {len(self.validation_loader)}')\n",
    "            m_val = len(self.validation_loader)\n",
    "            self.model.eval()\n",
    "            for batch_index, (inputs, targets) in enumerate(self.validation_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss_val, log_likelihood_val, kl_val = self.objective(outputs, targets, self.model.kl_divergence(), 1 / m_val)\n",
    "                v_total += targets.size(0)\n",
    "                v_loss.append(loss_val.item())\n",
    "                v_likelihood.append(log_likelihood_val.item())\n",
    "                v_kl.append(kl_val.item())\n",
    "\n",
    "            \n",
    "            avg_v_loss = np.average(v_loss)\n",
    "            avg_t_loss = np.average(t_loss)\n",
    "            avg_v_likelihood = np.average(v_likelihood)\n",
    "            avg_t_likelihood = np.average(t_likelihood)\n",
    "            avg_v_kl = np.average(v_kl)\n",
    "            avg_t_kl = np.average(t_kl)\n",
    "\n",
    "\n",
    "            print(\n",
    "                'epochs: {}, train loss: {}, train likelihood: {}, train kl: {}'.format(\n",
    "                    epochs, avg_t_loss, \\\n",
    "                    avg_t_likelihood, avg_t_kl))\n",
    "\n",
    "            print(\n",
    "                'epochs: {}, validation loss: {}, validation likelihood: {}, validation kl: {}'.format(\n",
    "                    epochs, avg_v_loss, \\\n",
    "                    avg_v_likelihood, avg_v_kl))\n",
    "\n",
    "            return avg_v_loss\n",
    "\n",
    "        else:\n",
    "            avg_t_loss = np.average(t_loss)\n",
    "            avg_t_likelihood = np.average(t_likelihood)\n",
    "            avg_t_kl = np.average(t_kl)\n",
    "            avg_t_r2 = np.average(t_r2_scores)\n",
    "\n",
    "         #   print(\n",
    "          #      'epochs: {}, train loss: {}, train likelihood: {}, train kl: {}, train_avg_R2: {}'.format(\n",
    "           #         epochs, avg_t_loss, \\\n",
    "            #        avg_t_likelihood, avg_t_kl, avg_t_r2))\n",
    "\n",
    "            return avg_t_loss, avg_t_r2\n",
    "\n",
    "    \n",
    "    def TestModel(self, rounds):\n",
    "        if device.type == 'cpu':\n",
    "            state = torch.load(self.train_weight_path, map_location=torch.device('cpu'))\n",
    "        else:\n",
    "            state = torch.load(self.train_weight_path)\n",
    "\n",
    "        self.model.load_state_dict(state['weights'])\n",
    "        print(f'Model loaded: {self.model}')\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        actual = []\n",
    "        mse_scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (inputs, targets) in enumerate(self.dataloader_test):\n",
    "                X, Y = inputs.to(device), targets.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                # Calculate the MSE loss for the batch\n",
    "                mse_loss = nn.MSELoss()\n",
    "                loss = mse_loss(outputs, Y)\n",
    "\n",
    "                # Get the MSE score as a Python scalar\n",
    "                mse_score = loss.item()\n",
    "                mse_scores.append(mse_score)\n",
    "\n",
    "                # Convert predictions and actual values to numpy arrays\n",
    "                predictions.append(outputs.detach().cpu().numpy())      \n",
    "                actual.append(Y.detach().cpu().numpy())\n",
    "                \n",
    "\n",
    "        predictions = np.concatenate(predictions)\n",
    "        actual = np.concatenate(actual)\n",
    "        df = pd.DataFrame(data = {'Predictions': predictions, 'Actual': actual})\n",
    "        df.loc['R2'] = 1 - np.sum((df.Actual - df.Predictions) ** 2) / np.sum((df.Actual - np.mean(df.Actual)) ** 2)\n",
    "        df.loc['MSE'] = mean_squared_error(df.Actual, df.Predictions)\n",
    "        \n",
    "        #print('Non-Ensemble Test MSE:{:.3f}, TestR2:{:.3f}'.format(df.loc[\"MSE\"][0], df.loc[\"R2\"][0]))\n",
    "                \n",
    "\n",
    "\n",
    "    def getTrainedModel(self, rounds):\n",
    "        # path to save the trained model\n",
    "        self.train_weight_path = 'trainedModels/trained_weights/' + self.model_name + '_' + 'e' + str(self.epochs) + '_' + '-r' + str(rounds) + '-b' + str(self.batch_size) + '.pkl'\n",
    "        return (self.model, self.train_weight_path)\n",
    "\n",
    "\n",
    "    def saveModel(self, model, optimizer, path_to_save):\n",
    "        state = {\n",
    "            'rounds': self.rounds,\n",
    "            'weights': model.state_dict(),\n",
    "            'selected_data': self.selected_data,\n",
    "            'optimizer': self.optimizer.state_dict()\n",
    "            }\n",
    "\n",
    "        path_to_save = 'trainedModels/trained_weights/' + self.model_name + '_' + 'e' + str(self.epochs) + '_' + '-r' + str(self.rounds) + '-b' + str(self.batch_size) + '.pkl'\n",
    "\n",
    "        torch.save(state, path_to_save)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/kristian/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([54, 1])) that is different to the input size (torch.Size([54])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 5, train loss: nan, train likelihood: nan, train kl: nan\n",
      "epochs: 5, validation loss: 136.45891571044922, validation likelihood: 1.1918407559394837, validation kl: 1352.670654296875\n",
      "Round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/kristian/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating selected data in round: 1\n",
      "Training model in round: 1\n",
      "epochs: 5, train loss: nan, train likelihood: nan, train kl: nan\n",
      "epochs: 5, validation loss: 136.4570327758789, validation likelihood: 1.1899572014808655, validation kl: 1352.670654296875\n",
      "Round: 2\n",
      "Annotating selected data in round: 2\n",
      "Length of the combined dataset: 6800, <torch.utils.data.dataset.ConcatDataset object at 0x150f65810>\n",
      "Length of the train loader: 107\n",
      "Training model in round: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining model in round: \u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m active_learning\u001b[39m.\u001b[39;49mTrainModel(r, \u001b[39m5\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[111], line 268\u001b[0m, in \u001b[0;36mrunActiveLearning.TrainModel\u001b[0;34m(self, rounds, epochs, is_validation)\u001b[0m\n\u001b[1;32m    265\u001b[0m t_r2_scores\u001b[39m.\u001b[39mappend(r2_score_value)\n\u001b[1;32m    267\u001b[0m t_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m--> 268\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    270\u001b[0m \u001b[39m# define the optimizer\u001b[39;00m\n\u001b[1;32m    271\u001b[0m optimizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DataViz-TTGQRQcT/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('trainedModels/trained_weights'):\n",
    "    os.makedirs('trainedModels/trained_weights')\n",
    "\n",
    "\n",
    "# use the class to run the active learning\n",
    "active_learning = runActiveLearning(model_name='simple', model=model, dataloader_train=dataset_train, top_unc = 500, dataloader_test=dataset_test, dataset_active_l= dataset_activeL, epochs=10, rounds=3, learning_rate=0.001, batch_size=64, instances = 30, seed_sample=4, retrain=False, resume_round=False, optimizer= torch.optim.Adam(model.parameters(), lr=0.001), df_custom = df_custom)\n",
    "\n",
    "write_summary = LogSummary('active_learning')\n",
    "\n",
    "# get data to train the model\n",
    "active_learning.get_validation_data(is_validation=True)\n",
    "\n",
    "# train just the seed model\n",
    "active_learning.TrainModel(1, 5, True)\n",
    "\n",
    "for r in range(1, active_learning.rounds):\n",
    "    print(f'Round: {r}')\n",
    "    active = active_learning.activeDataSelection(r)\n",
    "    \n",
    "    # annotate the selected data\n",
    "    print(f'Annotating selected data in round: {r}')\n",
    "    active_learning.annotateSelectedData(r)\n",
    "    # train the model\n",
    "    print(f'Training model in round: {r}')\n",
    "    active_learning.TrainModel(r, 5, True)\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
